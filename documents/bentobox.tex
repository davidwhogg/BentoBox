% This file is part of the Bento Box project.
% Copyright 2013 the authors.

\documentclass[12pt]{article}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\emcee}{\project{emcee}}

\newcommand{\mean}[1]{\left<{#1}\right>}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\data}{D}
\newcommand{\pars}{\theta}
\newcommand{\parspace}{\Omega}
\newcommand{\nwalker}{N_{\mathrm{w}}}
\newcommand{\ndim}{d}
\newcommand{\nburn}{N_{\mathrm{b}}}
\newcommand{\nsample}{N_{\mathrm{s}}}

\begin{document}

Briefly, the algorithm is:
\begin{enumerate}
\item \textbf{initialize} Start by specifying a likelihood
  $p(\data\given\pars)$ function and a prior $p(\pars)$, and a
  well-defined region $\parspace$ of parameter space.  The space
  $\parspace$ should be initialized to the full parameter space, but
  on recursive forks (see below), the space $\parspace$ will be a
  subspace of the full parameter space.  Or, another way to put it:
  The integral of the prior $p(\pars)$ over the entirety of
  $\parspace$ does not need to be unity, but it \emph{does} need to be
  finite.
\item \label{step:start}\textbf{start---sample prior:} Draw $\nwalker$
  samples from the prior, but restricted to the (possibly small sub-)
  space $\parspace$.  The number $\nwalker$ must be greater than
  something like twice the number $\ndim$ of dimensions of the space
  $\parspace$, if an ensemble sampler like \emcee\ is going to be
  used.
\item \textbf{run MCMC burn-in:} Initialize $\nwalker$ MCMC ``walkers''
  at the $\nwalker$ prior-sample points and run MCMC for $\nburn$
  iterations.  If this is performed with something like \emcee, this
  is done in one step.  If this is performed with something like
  Metropolis--Hastings, this is done in $\nwalker$ independent (though
  conceivably parallel) processes.
\item \textbf{compute mean walker properties:} At the end of the
  $\nburn$ iterations, compute the mean (over, say, the second half of
  each walker's chain) position $\mean{\pars_k}$ of every walker $k$.
  You could augment these $\ndim$-dimensional mean positions with
  acceptance ratios or mean likelihoods or anything else of interest.
\item \textbf{perform clustering hypothesis test:} Perform a
  clustering hypothesis test that indicates whether the mean position
  distribution is better described by one cluster or two.  This could
  be based on some kind of linear or Fisher discriminant analysis or
  perhaps comparison of mixtures of Gaussians with one and two
  Gaussians, using an information criterion based on how much the
  second Gaussian reduces the determinant of the total variance
  tensor.  The hypothesis test can be tuned to be conservative or
  aggressive; here we want to split the walkers only on good evidence.
  Given realities, the hypothesis test ought to be designed to be
  affine-invariant. The hypothesis test will either pass, indicating
  two clusters and returning labels $A$ and $B$, one unique label per
  walker, or else fail, indicating a single cluster of walkers.
\item \textbf{pass---recurse---split the space:} If the test passes,
  perform a kernel support vector machine classification of the space
  $\parspace$ using the labeled walkers and their mean positions as
  the training data.  The kernel of the kSVM could be set
  intelligently and in an affine-invariant way using the empirical
  variance of the samples in the clusters.  The output of this kSVM is
  effectively a pair of parameter subspaces $\parspace_A$ and
  $\parspace_B$ and a machine for classifying any new points.  Fork
  two processes, both of which start again at step~\ref{step:start} of
  this algorithm, but one of which sets
  $\parspace\leftarrow\parspace_A$, and the other of which sets
  $\parspace\leftarrow\parspace_B$.
\item \textbf{fail---compute the subspace marginalized likelihood:} If
  the test fails, compute the marginalized likelihood in the subspace
  \begin{eqnarray}
    p(\data\given\parspace)
    &\equiv&
    \int_\parspace p(\data\given\pars)\,p(\pars)\,\dd\pars
    \quad .
  \end{eqnarray}
  This integral can be computed, along with the production of a
  posterio sampling, by some very clever method produced by Goodman or
  Brewer or someone else.  This is \emph{relatively} easy because if
  this step is being executed, the subspace failed the two-cluster
  hypothesis test and is therefore relatively compact (in an
  affine-invariant sense).  Output the subspace identification
  information and a $\nsample$ sampling of the posterior within the
  subspace, and an estimate of the maginalized likelihood.
\item \textbf{finish---combine subspace samplings:} After all
  processes have finished forking and have produced marginalized
  likelihoods and samplings, the samplings can be combined into a
  single sampling.  This proceeds by importance sampling the
  individual-subspace samplings according to the individual-subspace
  marginalized likelihoods.
\end{enumerate}

\end{document}
